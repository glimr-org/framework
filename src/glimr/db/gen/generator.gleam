//// Code Generator
////
//// Writing repository boilerplate by hand — types, decoders,
//// query wrappers — is tedious and error-prone when schemas
//// change. This module generates all of that from the parsed
//// schema and SQL files, so a schema change only requires
//// re-running the generator instead of updating every type
//// and decoder manually.

import gleam/int
import gleam/list
import gleam/option.{None, Some}
import gleam/string
import glimr/db/gen/parser.{type ParsedQuery}
import glimr/db/gen/parser/columns.{type SelectedColumn}
import glimr/db/gen/schema_parser.{type ColumnType, type Table}
import glimr/db/gen/schema_parser/codegen

// ------------------------------------------------------------- Public Functions

/// Single entry point so callers don't need to know the order
/// or dependencies between header, imports, types, decoders,
/// and query functions. Each piece is generated independently
/// then joined, keeping the sub-generators composable.
///
pub fn generate(
  model_name: String,
  table: Table,
  queries: List(#(String, String, ParsedQuery)),
) -> String {
  let header = generate_header(model_name)
  let imports = generate_imports(table)
  let model_type = generate_model_type(model_name, table)
  let model_decoder = generate_model_decoder(model_name, table)
  let query_code = generate_queries(model_name, table, queries)

  string.join([header, imports, model_type, model_decoder, query_code], "\n\n")
}

// ------------------------------------------------------------- Private Functions

/// The DO NOT EDIT warning prevents developers from adding
/// hand-written code that would be silently overwritten the
/// next time the generator runs.
///
fn generate_header(model_name: String) -> String {
  "//// "
  <> pascal_case(model_name)
  <> " Model (GENERATED - DO NOT EDIT)\n////\n//// Generated by Glimr ✨"
}

/// Importing Option or glimr_decode when they're unused causes
/// Gleam compiler warnings. Checking the schema for nullable
/// and boolean columns ensures only the needed imports appear
/// in the generated file.
///
fn generate_imports(table: Table) -> String {
  let columns = schema_parser.columns(table)
  let has_boolean =
    list.any(columns, fn(col) { col.column_type == schema_parser.Boolean })
  let has_nullable = list.any(columns, fn(col) { col.nullable })

  let base_imports = "import gleam/dynamic/decode"

  let option_import = case has_nullable {
    True -> "\nimport gleam/option.{type Option}"
    False -> ""
  }

  let glimr_decode_import = case has_boolean {
    True -> "\nimport glimr/db/decode as glimr_decode"
    False -> ""
  }

  let db_import = "\nimport glimr/db/pool_connection as db"

  base_imports <> option_import <> glimr_decode_import <> db_import
}

/// The generated type mirrors the database table so rows
/// decoded from queries are fully typed. Nullable columns are
/// wrapped in Option to make missing values explicit at the
/// type level rather than using sentinel values.
///
fn generate_model_type(model_name: String, table: Table) -> String {
  let type_name = pascal_case(model_name)
  let columns = schema_parser.columns(table)

  let fields =
    list.map(columns, fn(col) {
      let gleam_type = codegen.gleam_type(col.column_type)
      let type_str = case col.nullable {
        True -> "Option(" <> gleam_type <> ")"
        False -> gleam_type
      }
      "    " <> col.name <> ": " <> type_str
    })

  "pub type "
  <> type_name
  <> " {\n  "
  <> type_name
  <> "(\n"
  <> string.join(fields, ",\n")
  <> ",\n  )\n}"
}

/// A public decoder lets query functions and custom code share
/// the same decoding logic. Using positional field indices
/// matches how database drivers return rows, and wrapping
/// nullable fields in decode.optional mirrors the Option types
/// in the model definition.
///
fn generate_model_decoder(model_name: String, table: Table) -> String {
  let type_name = pascal_case(model_name)
  let columns = schema_parser.columns(table)

  let field_decoders =
    list.index_map(columns, fn(col, idx) {
      let decoder = codegen.decoder_fn(col.column_type)
      let decoder_with_nullable = case col.nullable {
        True -> "decode.optional(" <> decoder <> ")"
        False -> decoder
      }
      "  use "
      <> col.name
      <> " <- decode.field("
      <> int.to_string(idx)
      <> ", "
      <> decoder_with_nullable
      <> ")"
    })

  let field_names = list.map(columns, fn(col) { col.name })
  let constructor_call =
    type_name <> "(" <> string.join(field_names, ", ") <> ")"

  "pub fn decoder() -> decode.Decoder("
  <> type_name
  <> ") {\n"
  <> string.join(field_decoders, "\n")
  <> "\n"
  <> "  decode.success("
  <> constructor_call
  <> ")\n}"
}

/// Separating the iteration from the per-query generation
/// keeps generate_single_query focused on one query at a time.
/// Blank lines between functions match Gleam's formatting
/// conventions.
///
fn generate_queries(
  model_name: String,
  table: Table,
  queries: List(#(String, String, ParsedQuery)),
) -> String {
  let query_codes =
    list.map(queries, fn(query_tuple) {
      let #(query_name, sql, parsed) = query_tuple
      generate_single_query(model_name, table, query_name, sql, parsed)
    })

  string.join(query_codes, "\n\n")
}

/// Queries that return rows need a decoder and possibly a
/// custom row type, while INSERT/UPDATE/DELETE only need an
/// exec wrapper. Branching here keeps the two code paths
/// independent. When a query's columns match the full model,
/// the existing model type and decoder are reused to avoid
/// generating duplicate types.
///
fn generate_single_query(
  model_name: String,
  table: Table,
  query_name: String,
  sql: String,
  parsed: ParsedQuery,
) -> String {
  let fn_name = snake_case(query_name)

  let columns_with_types = resolve_column_types(table, parsed.columns)

  let param_types = resolve_param_types(table, parsed.param_columns)

  case list.is_empty(columns_with_types) {
    True -> {
      generate_execute_function(fn_name, sql, parsed.params, param_types)
    }
    False -> {
      // Build model column signature for comparison
      let model_columns =
        list.map(schema_parser.columns(table), fn(col) {
          #(col.name, col.column_type, col.nullable)
        })

      // Check if query columns match the model exactly
      let matches_model = columns_with_types == model_columns

      case matches_model {
        True -> {
          // Reuse the model type and public decoder()
          let type_name = pascal_case(model_name)
          generate_query_function(
            fn_name,
            type_name,
            sql,
            parsed.params,
            columns_with_types,
            param_types,
            True,
          )
        }
        False -> {
          // Create a query-specific type named {Query}{Model}
          let type_name = pascal_case(query_name) <> pascal_case(model_name)

          let row_type = generate_row_type(type_name, columns_with_types)

          let row_decoder = generate_row_decoder(type_name, columns_with_types)

          let query_fn =
            generate_query_function(
              fn_name,
              type_name,
              sql,
              parsed.params,
              columns_with_types,
              param_types,
              False,
            )

          string.join([row_type, row_decoder, query_fn], "\n\n")
        }
      }
    }
  }
}

/// Generated functions need typed parameters (db.int vs
/// db.string) matching each column's schema type. BETWEEN
/// clauses produce start_/end_ prefixed names that don't
/// match schema columns directly, so the prefix is stripped
/// before lookup.
///
fn resolve_param_types(
  table: Table,
  param_columns: List(#(Int, String)),
) -> List(#(Int, String, ColumnType)) {
  let schema_columns = schema_parser.columns(table)

  list.filter_map(param_columns, fn(pc) {
    let #(param_num, col_name) = pc
    // Strip start_/end_ prefixes for BETWEEN params when looking up type
    let lookup_name = case string.starts_with(col_name, "start_") {
      True -> string.drop_start(col_name, 6)
      False ->
        case string.starts_with(col_name, "end_") {
          True -> string.drop_start(col_name, 4)
          False -> col_name
        }
    }
    case list.find(schema_columns, fn(sc) { sc.name == lookup_name }) {
      Ok(col) -> Ok(#(param_num, col_name, col.column_type))
      Error(_) -> Error(Nil)
    }
  })
}

/// The generated row type needs Gleam types for each selected
/// column. SELECT * must expand to all schema columns, aliases
/// must override column names, and aggregates like COUNT need
/// special type inference since they don't map to a schema
/// column directly.
///
fn resolve_column_types(
  table: Table,
  columns: List(SelectedColumn),
) -> List(#(String, ColumnType, Bool)) {
  let schema_columns = schema_parser.columns(table)

  // Check if there's a * (star) select - expand to all schema columns
  let has_star = list.any(columns, fn(sel_col) { sel_col.name == "*" })

  case has_star {
    True -> {
      // Expand * to all schema columns
      list.map(schema_columns, fn(col) {
        #(col.name, col.column_type, col.nullable)
      })
    }
    False -> {
      list.map(columns, fn(sel_col) {
        let name = case sel_col.alias {
          Some(alias) -> alias
          None -> sel_col.name
        }

        // First check if it's an aggregate function
        case infer_aggregate_type(sel_col.name) {
          Some(agg_type) -> #(name, agg_type, False)
          None -> {
            // Find matching column in schema
            let schema_col =
              list.find(schema_columns, fn(sc) { sc.name == sel_col.name })

            case schema_col {
              Ok(col) -> #(name, col.column_type, col.nullable)
              Error(_) -> #(name, schema_parser.String, False)
              // Default to String if not found
            }
          }
        }
      })
    }
  }
}

/// Aggregate functions return different types than their input
/// columns — COUNT always returns Int regardless of column
/// type, AVG always returns Float. Without this inference the
/// generator would assign the wrong Gleam type to aggregate
/// results.
///
fn infer_aggregate_type(expr: String) -> option.Option(ColumnType) {
  let upper = string.uppercase(expr)

  // COUNT always returns Int
  case string.starts_with(upper, "COUNT(") {
    True -> Some(schema_parser.Int)
    False -> {
      // SUM typically returns Int (could be Float for float columns)
      case string.starts_with(upper, "SUM(") {
        True -> Some(schema_parser.Int)
        False -> {
          // AVG returns Float
          case string.starts_with(upper, "AVG(") {
            True -> Some(schema_parser.Float)
            False -> None
          }
        }
      }
    }
  }
}

/// Queries that select a subset of columns can't reuse the
/// full model type since field count and order would differ.
/// A query-specific row type gives each partial select its
/// own typed result without forcing callers to ignore fields.
///
fn generate_row_type(
  type_name: String,
  columns: List(#(String, ColumnType, Bool)),
) -> String {
  let fields =
    list.map(columns, fn(col_tuple) {
      let #(name, col_type, nullable) = col_tuple
      let gleam_type = codegen.gleam_type(col_type)
      let type_str = case nullable {
        True -> "Option(" <> gleam_type <> ")"
        False -> gleam_type
      }
      "    " <> name <> ": " <> type_str
    })

  "pub type "
  <> type_name
  <> " {\n  "
  <> type_name
  <> "(\n"
  <> string.join(fields, ",\n")
  <> ",\n  )\n}"
}

/// Each query-specific row type needs its own decoder since
/// field indices and types differ from the model decoder.
/// Keeping it private avoids polluting the module's public API
/// with decoders that only the generated query function uses.
///
fn generate_row_decoder(
  type_name: String,
  columns: List(#(String, ColumnType, Bool)),
) -> String {
  let field_decoders =
    list.index_map(columns, fn(col_tuple, idx) {
      let #(name, col_type, nullable) = col_tuple
      let decoder = codegen.decoder_fn(col_type)
      let decoder_with_nullable = case nullable {
        True -> "decode.optional(" <> decoder <> ")"
        False -> decoder
      }
      "  use "
      <> name
      <> " <- decode.field("
      <> int.to_string(idx)
      <> ", "
      <> decoder_with_nullable
      <> ")"
    })

  let field_names =
    list.map(columns, fn(col_tuple) {
      let #(name, _, _) = col_tuple
      name
    })
  let constructor_call =
    type_name <> "(" <> string.join(field_names, ", ") <> ")"

  "fn "
  <> snake_case(type_name)
  <> "_decoder() -> decode.Decoder("
  <> type_name
  <> ") {\n"
  <> string.join(field_decoders, "\n")
  <> "\n"
  <> "  decode.success("
  <> constructor_call
  <> ")\n}"
}

/// Most callers just pass a Pool, but transaction blocks
/// already have a checked-out Connection. Generating both
/// variants lets callers use the convenient Pool version
/// normally and the _wc version inside transactions without
/// double-checking out a connection.
///
fn generate_query_function(
  fn_name: String,
  row_type_name: String,
  sql: String,
  params: List(Int),
  _columns: List(#(String, ColumnType, Bool)),
  param_types: List(#(Int, String, ColumnType)),
  uses_model_decoder: Bool,
) -> String {
  let param_count = list.length(params)

  // Detect if this is a multi-row query (list_*) or single-row (everything else)
  let is_single_row = !string.starts_with(fn_name, "list_")

  // Generate parameter list with proper types and labeled names
  let param_list = case param_count {
    0 -> ""
    _ ->
      ", "
      <> string.join(
        int.range(from: 1, to: param_count + 1, with: [], run: fn(acc, n) {
          [
            case list.find(param_types, fn(pt) { pt.0 == n }) {
              Ok(#(_, col_name, col_type)) -> {
                let gleam_type = codegen.gleam_type(col_type)
                col_name <> " " <> col_name <> ": " <> gleam_type
              }
              Error(_) -> {
                let name = "p" <> int.to_string(n)
                name <> " " <> name <> ": String"
              }
            },
            ..acc
          ]
        })
          |> list.reverse,
        ", ",
      )
  }

  // Generate parameter values using db.int/db.string/etc.
  let param_values = case param_count {
    0 -> "[]"
    _ ->
      "["
      <> string.join(
        int.range(from: 1, to: param_count + 1, with: [], run: fn(acc, n) {
          [
            case list.find(param_types, fn(pt) { pt.0 == n }) {
              Ok(#(_, col_name, col_type)) -> {
                let wrapper = value_wrapper(col_type)
                wrapper <> "(" <> col_name <> ")"
              }
              Error(_) -> "db.string(p" <> int.to_string(n) <> ")"
            },
            ..acc
          ]
        })
          |> list.reverse,
        ", ",
      )
      <> "]"
  }

  // Generate param names for passing to _wc function (with labels)
  let param_names = case param_count {
    0 -> ""
    _ ->
      ", "
      <> string.join(
        int.range(from: 1, to: param_count + 1, with: [], run: fn(acc, n) {
          [
            case list.find(param_types, fn(pt) { pt.0 == n }) {
              Ok(#(_, col_name, _)) -> col_name <> ": " <> col_name
              Error(_) -> {
                let name = "p" <> int.to_string(n)
                name <> ": " <> name
              }
            },
            ..acc
          ]
        })
          |> list.reverse,
        ", ",
      )
  }

  // Strip comments and escape the SQL for Gleam string
  let escaped_sql = escape_string(strip_sql_comments(sql))
  let decoder_fn = case uses_model_decoder {
    True -> "decoder()"
    False -> snake_case(row_type_name) <> "_decoder()"
  }

  // Generate _wc (with connection) variant using db.query_with
  let wc_fn =
    generate_wc_query(
      fn_name,
      row_type_name,
      escaped_sql,
      param_values,
      decoder_fn,
      param_list,
      is_single_row,
    )

  // Generate main function that accepts Pool using get_connection
  let main_fn = case is_single_row {
    True ->
      "pub fn "
      <> fn_name
      <> "(pool pool: db.Pool"
      <> param_list
      <> ") -> Result("
      <> row_type_name
      <> ", db.DbError) {\n"
      <> "  use connection <- db.get_connection(pool)\n"
      <> "  "
      <> fn_name
      <> "_wc(connection: connection"
      <> param_names
      <> ")\n}"
    False ->
      "pub fn "
      <> fn_name
      <> "(pool pool: db.Pool"
      <> param_list
      <> ") -> Result(List("
      <> row_type_name
      <> "), db.DbError) {\n"
      <> "  use connection <- db.get_connection(pool)\n"
      <> "  "
      <> fn_name
      <> "_wc(connection: connection"
      <> param_names
      <> ")\n}"
  }

  string.join([main_fn, wc_fn], "\n\n")
}

/// Separated from generate_query_function to isolate the _wc
/// template from the parameter-building logic. Single-row
/// queries match on exactly one result and return NotFound for
/// empty results, while list queries pass all rows through.
///
fn generate_wc_query(
  fn_name: String,
  row_type_name: String,
  escaped_sql: String,
  param_values: String,
  decoder_fn: String,
  param_list: String,
  is_single_row: Bool,
) -> String {
  case is_single_row {
    True ->
      "pub fn "
      <> fn_name
      <> "_wc(connection connection: db.Connection"
      <> param_list
      <> ") -> Result("
      <> row_type_name
      <> ", db.DbError) {\n"
      <> "  case db.query_with(connection, \""
      <> escaped_sql
      <> "\", "
      <> param_values
      <> ", "
      <> decoder_fn
      <> ") {\n"
      <> "    Ok(db.QueryResult(_, [row])) -> Ok(row)\n"
      <> "    Ok(db.QueryResult(_, [])) -> Error(db.NotFound)\n"
      <> "    Ok(_) -> Error(db.QueryError(\"Expected single row\"))\n"
      <> "    Error(e) -> Error(e)\n"
      <> "  }\n}"
    False ->
      "pub fn "
      <> fn_name
      <> "_wc(connection connection: db.Connection"
      <> param_list
      <> ") -> Result(List("
      <> row_type_name
      <> "), db.DbError) {\n"
      <> "  case db.query_with(connection, \""
      <> escaped_sql
      <> "\", "
      <> param_values
      <> ", "
      <> decoder_fn
      <> ") {\n"
      <> "    Ok(db.QueryResult(_, rows)) -> Ok(rows)\n"
      <> "    Error(e) -> Error(e)\n"
      <> "  }\n}"
  }
}

/// Write operations don't return rows so they use db.exec_with
/// instead of db.query_with, avoiding the need for a decoder.
/// The same Pool/_wc dual-function pattern as query functions
/// gives transaction support without a separate API.
///
fn generate_execute_function(
  fn_name: String,
  sql: String,
  params: List(Int),
  param_types: List(#(Int, String, ColumnType)),
) -> String {
  let param_count = list.length(params)

  // Generate parameter list with proper types and labeled names
  let param_list = case param_count {
    0 -> ""
    _ ->
      ", "
      <> string.join(
        int.range(from: 1, to: param_count + 1, with: [], run: fn(acc, n) {
          [
            case list.find(param_types, fn(pt) { pt.0 == n }) {
              Ok(#(_, col_name, col_type)) -> {
                let gleam_type = codegen.gleam_type(col_type)
                col_name <> " " <> col_name <> ": " <> gleam_type
              }
              Error(_) -> {
                let name = "p" <> int.to_string(n)
                name <> " " <> name <> ": String"
              }
            },
            ..acc
          ]
        })
          |> list.reverse,
        ", ",
      )
  }

  // Generate param names for passing to _wc function (with labels)
  let param_names = case param_count {
    0 -> ""
    _ ->
      ", "
      <> string.join(
        int.range(from: 1, to: param_count + 1, with: [], run: fn(acc, n) {
          [
            case list.find(param_types, fn(pt) { pt.0 == n }) {
              Ok(#(_, col_name, _)) -> col_name <> ": " <> col_name
              Error(_) -> {
                let name = "p" <> int.to_string(n)
                name <> ": " <> name
              }
            },
            ..acc
          ]
        })
          |> list.reverse,
        ", ",
      )
  }

  // Generate parameter values using db.int/db.string/etc.
  let param_values = case param_count {
    0 -> "[]"
    _ ->
      "["
      <> string.join(
        int.range(from: 1, to: param_count + 1, with: [], run: fn(acc, n) {
          [
            case list.find(param_types, fn(pt) { pt.0 == n }) {
              Ok(#(_, col_name, col_type)) -> {
                let wrapper = value_wrapper(col_type)
                wrapper <> "(" <> col_name <> ")"
              }
              Error(_) -> "db.string(p" <> int.to_string(n) <> ")"
            },
            ..acc
          ]
        })
          |> list.reverse,
        ", ",
      )
      <> "]"
  }

  // Strip comments and escape the SQL for Gleam string
  let escaped_sql = escape_string(strip_sql_comments(sql))

  // Generate _wc (with connection) variant using db.exec_with
  let wc_fn =
    generate_wc_execute(fn_name, escaped_sql, param_values, param_list)

  // Generate main function that accepts Pool using get_connection
  let main_fn =
    "pub fn "
    <> fn_name
    <> "(pool pool: db.Pool"
    <> param_list
    <> ") -> Result(Int, db.DbError) {\n"
    <> "  use connection <- db.get_connection(pool)\n"
    <> "  "
    <> fn_name
    <> "_wc(connection: connection"
    <> param_names
    <> ")\n}"

  string.join([main_fn, wc_fn], "\n\n")
}

/// Separated from generate_execute_function to isolate the _wc
/// template from the parameter-building logic. Returning the
/// affected row count lets callers detect "0 rows updated"
/// scenarios without a separate existence check.
///
fn generate_wc_execute(
  fn_name: String,
  escaped_sql: String,
  param_values: String,
  param_list: String,
) -> String {
  "pub fn "
  <> fn_name
  <> "_wc(connection connection: db.Connection"
  <> param_list
  <> ") -> Result(Int, db.DbError) {\n"
  <> "  db.exec_with(connection, \""
  <> escaped_sql
  <> "\", "
  <> param_values
  <> ")\n}"
}

/// Generated SQL parameters need the right db.* wrapper to
/// encode Gleam values for the driver. This mapping ensures
/// each column type gets the correct wrapper so type mismatches
/// are caught at compile time rather than at query execution.
///
fn value_wrapper(col_type: ColumnType) -> String {
  case col_type {
    schema_parser.Id -> "db.int"
    schema_parser.String -> "db.string"
    schema_parser.Text -> "db.string"
    schema_parser.Int -> "db.int"
    schema_parser.BigInt -> "db.int"
    schema_parser.Float -> "db.float"
    schema_parser.Boolean -> "db.bool"
    schema_parser.Timestamp -> "db.string"
    schema_parser.UnixTimestamp -> "db.int"
    schema_parser.Date -> "db.string"
    schema_parser.Json -> "db.string"
    schema_parser.Uuid -> "db.string"
    schema_parser.Foreign(_) -> "db.int"
  }
}

/// Gleam types must be PascalCase but model names from the
/// schema are snake_case. Converting here keeps the naming
/// convention consistent with Gleam's style in generated code.
///
fn pascal_case(s: String) -> String {
  s
  |> string.split("_")
  |> list.map(capitalize)
  |> string.join("")
}

/// Gleam functions must be snake_case but composite type names
/// are built in PascalCase. Converting back ensures generated
/// decoder function names follow Gleam's naming conventions.
///
fn snake_case(s: String) -> String {
  do_snake_case(string.to_graphemes(s), "", False)
}

/// Gleam lacks regex or imperative loops, so case conversion
/// requires manual recursion. Tracking the previous character's
/// case determines where to insert underscores — only between
/// a lowercase-to-uppercase transition like "fooBar" → "foo_bar".
///
fn do_snake_case(chars: List(String), acc: String, prev_lower: Bool) -> String {
  case chars {
    [] -> string.lowercase(acc)
    [c, ..rest] -> {
      let is_upper = c == string.uppercase(c) && c != string.lowercase(c)
      case is_upper && prev_lower {
        True -> do_snake_case(rest, acc <> "_" <> c, False)
        False -> do_snake_case(rest, acc <> c, !is_upper)
      }
    }
  }
}

/// Gleam's standard library doesn't have a capitalize function.
/// This is needed by pascal_case to uppercase each segment's
/// first letter while preserving the rest.
///
fn capitalize(s: String) -> String {
  case string.pop_grapheme(s) {
    Ok(#(first, rest)) -> string.uppercase(first) <> rest
    Error(_) -> s
  }
}

/// SQL strings embedded in generated Gleam code must be valid
/// string literals. Unescaped quotes or backslashes would
/// produce syntax errors in the generated file, and newlines
/// would break single-line string literals.
///
fn escape_string(s: String) -> String {
  s
  |> string.replace("\\", "\\\\")
  |> string.replace("\"", "\\\"")
  |> string.replace("\n", " ")
}

/// SQL comments are useful in the source .sql files but waste
/// space and clutter the generated code. Stripping them before
/// embedding produces cleaner generated modules and shorter
/// SQL strings sent to the database.
///
fn strip_sql_comments(sql: String) -> String {
  sql
  |> strip_block_comments()
  |> strip_line_comments()
  |> collapse_whitespace()
}

/// Block comments can span multiple lines and may appear
/// anywhere in the SQL. Recursive removal handles the case
/// where stripping one comment reveals another that was
/// partially inside it.
///
fn strip_block_comments(sql: String) -> String {
  case string.split_once(sql, "/*") {
    Error(_) -> sql
    Ok(#(before, after)) -> {
      case string.split_once(after, "*/") {
        Error(_) -> before
        Ok(#(_, rest)) -> strip_block_comments(before <> " " <> rest)
      }
    }
  }
}

/// Line comments run to end-of-line, so splitting on newlines
/// first then truncating at "--" cleanly removes them without
/// affecting "--" inside string literals (which the SQL parser
/// handles separately).
///
fn strip_line_comments(sql: String) -> String {
  sql
  |> string.split("\n")
  |> list.map(fn(line) {
    case string.split_once(line, "--") {
      Error(_) -> line
      Ok(#(before, _)) -> before
    }
  })
  |> string.join(" ")
}

/// Comment removal and line joining can leave runs of spaces
/// that make the embedded SQL harder to read and slightly
/// larger. Collapsing to single spaces produces clean,
/// compact SQL in the generated output.
///
fn collapse_whitespace(sql: String) -> String {
  let collapsed = string.replace(sql, "  ", " ")
  case collapsed == sql {
    True -> string.trim(sql)
    False -> collapse_whitespace(collapsed)
  }
}
