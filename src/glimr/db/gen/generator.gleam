//// Model Code Generator
////
//// Every time you add a column to a table, you'd normally need
//// to update the type definition, the row decoder, the JSON
//// encoder, the JSON decoder, and every query function that
//// touches that table. This module does all of that
//// automatically — it reads the parsed schema and SQL query
//// files and spits out complete, ready-to-use Gleam modules
//// with types, decoders, encoders, and query wrappers.

import gleam/int
import gleam/list
import gleam/option.{None, Some}
import gleam/string
import glimr/db/gen/parser.{type ParsedQuery}
import glimr/db/gen/parser/columns.{type SelectedColumn}
import glimr/db/gen/schema_parser.{type ColumnType, type Table}
import glimr/db/gen/schema_parser/codegen

// ------------------------------------------------------------- Public Functions

/// One call produces an entire Gleam module — header, imports,
/// model type, row decoder, JSON encoder/decoder, and all query
/// functions. Callers don't need to worry about ordering or
/// which pieces depend on which; this function handles the
/// assembly so the build step is just "parse schema, parse
/// queries, generate."
///
pub fn generate(
  model_name: String,
  table: Table,
  queries: List(#(String, String, ParsedQuery)),
) -> String {
  let header = generate_header(model_name)
  let imports = generate_imports(table)
  let model_type = generate_model_type(model_name, table)
  let model_row_decoder = generate_model_decoder(model_name, table)
  let model_encoder = generate_model_encoder(model_name, table)
  let model_decoder = generate_model_json_decoder(model_name, table)
  let query_code = generate_queries(model_name, table, queries)

  string.join(
    [
      header,
      imports,
      model_type,
      model_row_decoder,
      model_encoder,
      model_decoder,
      query_code,
    ],
    "\n\n",
  )
}

// ------------------------------------------------------------- Private Functions

/// Without the "DO NOT EDIT" warning, developers would add
/// custom methods to the generated file and then lose them the
/// next time someone runs the generator. The warning makes it
/// obvious that changes belong in a separate module or in the
/// SQL files that feed the generator.
///
fn generate_header(model_name: String) -> String {
  "//// "
  <> pascal_case(model_name)
  <> " Model (GENERATED - DO NOT EDIT)\n////\n//// Generated by Glimr ✨"
}

/// Gleam's compiler treats unused imports as warnings, and
/// developers who see warnings in generated code start
/// distrusting the generator. Checking whether the table
/// actually has nullable or boolean columns before adding those
/// imports keeps the generated output warning-free.
///
fn generate_imports(table: Table) -> String {
  let columns = schema_parser.columns(table)
  let has_boolean =
    list.any(columns, fn(col) { col.column_type == schema_parser.Boolean })
  let has_nullable = list.any(columns, fn(col) { col.nullable })

  let base_imports = "import gleam/dynamic/decode\nimport gleam/json"

  let option_import = case has_nullable {
    True -> "\nimport gleam/option.{type Option}"
    False -> ""
  }

  let glimr_decode_import = case has_boolean {
    True -> "\nimport glimr/db/decode as glimr_decode"
    False -> ""
  }

  let db_import = "\nimport glimr/db/pool_connection as db"

  base_imports <> option_import <> glimr_decode_import <> db_import
}

/// The whole point of code generation is type safety — instead
/// of working with raw dynamic rows, developers get a proper
/// Gleam type with named fields. Nullable columns become Option
/// types so the compiler forces you to handle the None case
/// instead of crashing on an unexpected Nil at runtime.
///
fn generate_model_type(model_name: String, table: Table) -> String {
  let type_name = pascal_case(model_name)
  let columns = schema_parser.columns(table)

  let fields =
    list.map(columns, fn(col) {
      let gleam_type = codegen.gleam_type(col.column_type)
      let type_str = case col.nullable {
        True -> "Option(" <> gleam_type <> ")"
        False -> gleam_type
      }
      "    " <> col.name <> ": " <> type_str
    })

  "pub type "
  <> type_name
  <> " {\n  "
  <> type_name
  <> "(\n"
  <> string.join(fields, ",\n")
  <> ",\n  )\n}"
}

/// Database drivers return rows as positional arrays, not named
/// fields — so the decoder uses integer indices (field 0, field
/// 1, etc.) to pull values in the right order. This is private
/// because it's an implementation detail that only the
/// generated query functions need; the public decoder() uses
/// named fields for JSON deserialization instead.
///
fn generate_model_decoder(model_name: String, table: Table) -> String {
  let type_name = pascal_case(model_name)
  let columns = schema_parser.columns(table)

  let field_decoders =
    list.index_map(columns, fn(col, idx) {
      let decoder = codegen.decoder_fn(col.column_type)
      let decoder_with_nullable = case col.nullable {
        True -> "decode.optional(" <> decoder <> ")"
        False -> decoder
      }
      "  use "
      <> col.name
      <> " <- decode.field("
      <> int.to_string(idx)
      <> ", "
      <> decoder_with_nullable
      <> ")"
    })

  let field_names = list.map(columns, fn(col) { col.name })
  let constructor_call =
    type_name <> "(" <> string.join(field_names, ", ") <> ")"

  "fn row_decoder() -> decode.Decoder("
  <> type_name
  <> ") {\n"
  <> string.join(field_decoders, "\n")
  <> "\n"
  <> "  decode.success("
  <> constructor_call
  <> ")\n}"
}

/// API endpoints need to serialize models to JSON, and writing
/// the encoder by hand means keeping it in sync every time a
/// column changes. Generating it automatically guarantees every
/// field is encoded with the right json.* function, and
/// nullable fields get json.nullable so Option values serialize
/// as null instead of crashing.
///
fn generate_model_encoder(model_name: String, table: Table) -> String {
  let type_name = pascal_case(model_name)
  let columns = schema_parser.columns(table)

  let fields =
    list.map(columns, fn(col) {
      let encoder = codegen.json_encoder_fn(col.column_type)
      case col.nullable {
        True ->
          "    #(\""
          <> col.name
          <> "\", json.nullable(model."
          <> col.name
          <> ", "
          <> encoder
          <> "))"
        False ->
          "    #(\""
          <> col.name
          <> "\", "
          <> encoder
          <> "(model."
          <> col.name
          <> "))"
      }
    })

  "pub fn encoder() -> fn("
  <> type_name
  <> ") -> json.Json {\n"
  <> "  fn(model: "
  <> type_name
  <> ") {\n"
  <> "    json.object([\n"
  <> string.join(fields, ",\n")
  <> ",\n    ])\n  }\n}"
}

/// When your API receives JSON (e.g. from a frontend or
/// webhook), you need a decoder that looks up fields by name
/// like "email" and "created_at" — not by position like the row
/// decoder does. Having both means the same model type works
/// for database queries and JSON parsing without any manual
/// conversion code.
///
fn generate_model_json_decoder(model_name: String, table: Table) -> String {
  let type_name = pascal_case(model_name)
  let columns = schema_parser.columns(table)

  let field_decoders =
    list.map(columns, fn(col) {
      let decoder = codegen.decoder_fn(col.column_type)
      let decoder_with_nullable = case col.nullable {
        True -> "decode.optional(" <> decoder <> ")"
        False -> decoder
      }
      "  use "
      <> col.name
      <> " <- decode.field(\""
      <> col.name
      <> "\", "
      <> decoder_with_nullable
      <> ")"
    })

  let field_names = list.map(columns, fn(col) { col.name })
  let constructor_call =
    type_name <> "(" <> string.join(field_names, ", ") <> ")"

  "pub fn decoder() -> decode.Decoder("
  <> type_name
  <> ") {\n"
  <> string.join(field_decoders, "\n")
  <> "\n"
  <> "  decode.success("
  <> constructor_call
  <> ")\n}"
}

/// Each .sql file in the queries directory becomes a typed
/// function in the generated module. This maps over all of them
/// and joins the output with blank lines so the generated file
/// follows Gleam's formatting conventions.
///
fn generate_queries(
  model_name: String,
  table: Table,
  queries: List(#(String, String, ParsedQuery)),
) -> String {
  let query_codes =
    list.map(queries, fn(query_tuple) {
      let #(query_name, sql, parsed) = query_tuple
      generate_single_query(model_name, table, query_name, sql, parsed)
    })

  string.join(query_codes, "\n\n")
}

/// A SELECT * query can reuse the model's type and decoder
/// directly, but a SELECT id, name query needs its own custom
/// row type — otherwise the field count wouldn't match and the
/// decoder would crash. INSERT/UPDATE/DELETE queries don't
/// return rows at all, so they get a simpler exec wrapper
/// instead. This function figures out which case applies and
/// generates the right code.
///
fn generate_single_query(
  model_name: String,
  table: Table,
  query_name: String,
  sql: String,
  parsed: ParsedQuery,
) -> String {
  let fn_name = snake_case(query_name)

  let columns_with_types = resolve_column_types(table, parsed.columns)

  let param_types = resolve_param_types(table, parsed.param_columns)

  case list.is_empty(columns_with_types) {
    True -> {
      generate_execute_function(fn_name, sql, parsed.params, param_types)
    }
    False -> {
      // Build model column signature for comparison
      let model_columns =
        list.map(schema_parser.columns(table), fn(col) {
          #(col.name, col.column_type, col.nullable)
        })

      // Check if query columns match the model exactly
      let matches_model = columns_with_types == model_columns

      case matches_model {
        True -> {
          // Reuse the model type and row_decoder()
          let type_name = pascal_case(model_name)
          generate_query_function(
            fn_name,
            type_name,
            sql,
            parsed.params,
            columns_with_types,
            param_types,
            True,
          )
        }
        False -> {
          // Create a query-specific type named {Query}{Model}
          let type_name = pascal_case(query_name) <> pascal_case(model_name)

          let row_type = generate_row_type(type_name, columns_with_types)

          let row_decoder = generate_row_decoder(type_name, columns_with_types)

          let row_encoder = generate_row_encoder(type_name, columns_with_types)

          let row_json_decoder =
            generate_row_json_decoder(type_name, columns_with_types)

          let query_fn =
            generate_query_function(
              fn_name,
              type_name,
              sql,
              parsed.params,
              columns_with_types,
              param_types,
              False,
            )

          string.join(
            [row_type, row_decoder, row_encoder, row_json_decoder, query_fn],
            "\n\n",
          )
        }
      }
    }
  }
}

/// Query parameters need the right db.int/db.string wrapper so
/// the compiler catches type mismatches — passing a String
/// where the column expects Int would fail at runtime
/// otherwise. BETWEEN clauses are tricky because they produce
/// params like start_created_at and end_created_at, which don't
/// match the schema column name directly. Stripping the prefix
/// before lookup handles that transparently.
///
fn resolve_param_types(
  table: Table,
  param_columns: List(#(Int, String)),
) -> List(#(Int, String, ColumnType)) {
  let schema_columns = schema_parser.columns(table)

  list.filter_map(param_columns, fn(pc) {
    let #(param_num, col_name) = pc
    // Strip start_/end_ prefixes for BETWEEN params when looking up type
    let lookup_name = case string.starts_with(col_name, "start_") {
      True -> string.drop_start(col_name, 6)
      False ->
        case string.starts_with(col_name, "end_") {
          True -> string.drop_start(col_name, 4)
          False -> col_name
        }
    }
    case list.find(schema_columns, fn(sc) { sc.name == lookup_name }) {
      Ok(col) -> Ok(#(param_num, col_name, col.column_type))
      Error(_) -> Error(Nil)
    }
  })
}

/// Figuring out what type each selected column should be is
/// surprisingly involved. SELECT * needs to expand to all
/// schema columns, aliases like "SELECT name AS username" need
/// to use the alias as the field name, and aggregates like
/// COUNT(*) need special handling because they return Int
/// regardless of what column they operate on. Getting any of
/// these wrong produces a decoder that crashes.
///
fn resolve_column_types(
  table: Table,
  columns: List(SelectedColumn),
) -> List(#(String, ColumnType, Bool)) {
  let schema_columns = schema_parser.columns(table)

  // Check if there's a * (star) select - expand to all schema columns
  let has_star = list.any(columns, fn(sel_col) { sel_col.name == "*" })

  case has_star {
    True -> {
      // Expand * to all schema columns
      list.map(schema_columns, fn(col) {
        #(col.name, col.column_type, col.nullable)
      })
    }
    False -> {
      list.map(columns, fn(sel_col) {
        let name = case sel_col.alias {
          Some(alias) -> alias
          None -> sel_col.name
        }

        // First check if it's an aggregate function
        case infer_aggregate_type(sel_col.name) {
          Some(agg_type) -> #(name, agg_type, False)
          None -> {
            // Find matching column in schema
            let schema_col =
              list.find(schema_columns, fn(sc) { sc.name == sel_col.name })

            case schema_col {
              Ok(col) -> #(name, col.column_type, col.nullable)
              Error(_) -> #(name, schema_parser.String, False)
              // Default to String if not found
            }
          }
        }
      })
    }
  }
}

/// COUNT(*) returns an integer even if it's counting a text
/// column, and AVG always returns a float even on integer
/// columns. Without special-casing these, the generator would
/// look up "COUNT(id)" in the schema, not find it, and default
/// to String — which would silently produce a decoder that
/// fails at runtime.
///
fn infer_aggregate_type(expr: String) -> option.Option(ColumnType) {
  let upper = string.uppercase(expr)

  // COUNT always returns Int
  case string.starts_with(upper, "COUNT(") {
    True -> Some(schema_parser.Int)
    False -> {
      // SUM typically returns Int (could be Float for float columns)
      case string.starts_with(upper, "SUM(") {
        True -> Some(schema_parser.Int)
        False -> {
          // AVG returns Float
          case string.starts_with(upper, "AVG(") {
            True -> Some(schema_parser.Float)
            False -> None
          }
        }
      }
    }
  }
}

/// A query like SELECT id, name FROM users can't return the
/// full User type because it's missing half the fields. This
/// generates a query-specific type like FindByNameUser with
/// just the columns the query actually returns, so callers get
/// proper type safety instead of a User with half the fields
/// set to default values.
///
fn generate_row_type(
  type_name: String,
  columns: List(#(String, ColumnType, Bool)),
) -> String {
  let fields =
    list.map(columns, fn(col_tuple) {
      let #(name, col_type, nullable) = col_tuple
      let gleam_type = codegen.gleam_type(col_type)
      let type_str = case nullable {
        True -> "Option(" <> gleam_type <> ")"
        False -> gleam_type
      }
      "    " <> name <> ": " <> type_str
    })

  "pub type "
  <> type_name
  <> " {\n  "
  <> type_name
  <> "(\n"
  <> string.join(fields, ",\n")
  <> ",\n  )\n}"
}

/// Each custom row type needs its own positional decoder
/// because the field indices are different from the full model.
/// This is kept private — it's an implementation detail that
/// only the generated query function calls. The public decoder
/// for JSON uses named fields and is generated separately.
///
fn generate_row_decoder(
  type_name: String,
  columns: List(#(String, ColumnType, Bool)),
) -> String {
  let field_decoders =
    list.index_map(columns, fn(col_tuple, idx) {
      let #(name, col_type, nullable) = col_tuple
      let decoder = codegen.decoder_fn(col_type)
      let decoder_with_nullable = case nullable {
        True -> "decode.optional(" <> decoder <> ")"
        False -> decoder
      }
      "  use "
      <> name
      <> " <- decode.field("
      <> int.to_string(idx)
      <> ", "
      <> decoder_with_nullable
      <> ")"
    })

  let field_names =
    list.map(columns, fn(col_tuple) {
      let #(name, _, _) = col_tuple
      name
    })
  let constructor_call =
    type_name <> "(" <> string.join(field_names, ", ") <> ")"

  "fn "
  <> snake_case(type_name)
  <> "_row_decoder() -> decode.Decoder("
  <> type_name
  <> ") {\n"
  <> string.join(field_decoders, "\n")
  <> "\n"
  <> "  decode.success("
  <> constructor_call
  <> ")\n}"
}

/// When an API endpoint uses a partial query (SELECT id, name
/// instead of SELECT *), it still needs to serialize the result
/// to JSON. This generates an encoder specific to that query's
/// row type, following Gleam's snake_case naming so it looks
/// natural alongside hand-written code.
///
fn generate_row_encoder(
  type_name: String,
  columns: List(#(String, ColumnType, Bool)),
) -> String {
  let fn_name = snake_case(type_name)

  let fields =
    list.map(columns, fn(col_tuple) {
      let #(name, col_type, nullable) = col_tuple
      let encoder = codegen.json_encoder_fn(col_type)
      case nullable {
        True ->
          "    #(\""
          <> name
          <> "\", json.nullable(model."
          <> name
          <> ", "
          <> encoder
          <> "))"
        False ->
          "    #(\"" <> name <> "\", " <> encoder <> "(model." <> name <> "))"
      }
    })

  "pub fn "
  <> fn_name
  <> "_encoder() -> fn("
  <> type_name
  <> ") -> json.Json {\n"
  <> "  fn(model: "
  <> type_name
  <> ") {\n"
  <> "    json.object([\n"
  <> string.join(fields, ",\n")
  <> ",\n    ])\n  }\n}"
}

/// Same idea as generate_model_json_decoder but for
/// query-specific row types. If your API caches partial query
/// results as JSON, you need a decoder that can reconstruct the
/// row type from named fields — positional indices won't work
/// for JSON objects.
///
fn generate_row_json_decoder(
  type_name: String,
  columns: List(#(String, ColumnType, Bool)),
) -> String {
  let fn_name = snake_case(type_name)

  let field_decoders =
    list.map(columns, fn(col_tuple) {
      let #(name, col_type, nullable) = col_tuple
      let decoder = codegen.decoder_fn(col_type)
      let decoder_with_nullable = case nullable {
        True -> "decode.optional(" <> decoder <> ")"
        False -> decoder
      }
      "  use "
      <> name
      <> " <- decode.field(\""
      <> name
      <> "\", "
      <> decoder_with_nullable
      <> ")"
    })

  let field_names =
    list.map(columns, fn(col_tuple) {
      let #(name, _, _) = col_tuple
      name
    })
  let constructor_call =
    type_name <> "(" <> string.join(field_names, ", ") <> ")"

  "pub fn "
  <> fn_name
  <> "_decoder() -> decode.Decoder("
  <> type_name
  <> ") {\n"
  <> string.join(field_decoders, "\n")
  <> "\n"
  <> "  decode.success("
  <> constructor_call
  <> ")\n}"
}

/// Every query gets two generated functions: one that takes a
/// Pool (convenient for normal use) and a _wc variant that
/// takes a Connection directly (for use inside transactions).
/// Without the _wc variant, transaction blocks would have to
/// check out a second connection from the pool, which defeats
/// the purpose of the transaction. The list_ prefix convention
/// determines whether the function returns a single row or a
/// list.
///
fn generate_query_function(
  fn_name: String,
  row_type_name: String,
  sql: String,
  params: List(Int),
  _columns: List(#(String, ColumnType, Bool)),
  param_types: List(#(Int, String, ColumnType)),
  uses_model_decoder: Bool,
) -> String {
  let param_count = list.length(params)

  // Detect if this is a multi-row query (list_*) or single-row (everything else)
  let is_single_row = !string.starts_with(fn_name, "list_")

  // Generate parameter list with proper types and labeled names
  let param_list = case param_count {
    0 -> ""
    _ ->
      ", "
      <> string.join(
        int.range(from: 1, to: param_count + 1, with: [], run: fn(acc, n) {
          [
            case list.find(param_types, fn(pt) { pt.0 == n }) {
              Ok(#(_, col_name, col_type)) -> {
                let gleam_type = codegen.gleam_type(col_type)
                col_name <> " " <> col_name <> ": " <> gleam_type
              }
              Error(_) -> {
                let name = "p" <> int.to_string(n)
                name <> " " <> name <> ": String"
              }
            },
            ..acc
          ]
        })
          |> list.reverse,
        ", ",
      )
  }

  // Generate parameter values using db.int/db.string/etc.
  let param_values = case param_count {
    0 -> "[]"
    _ ->
      "["
      <> string.join(
        int.range(from: 1, to: param_count + 1, with: [], run: fn(acc, n) {
          [
            case list.find(param_types, fn(pt) { pt.0 == n }) {
              Ok(#(_, col_name, col_type)) -> {
                let wrapper = value_wrapper(col_type)
                wrapper <> "(" <> col_name <> ")"
              }
              Error(_) -> "db.string(p" <> int.to_string(n) <> ")"
            },
            ..acc
          ]
        })
          |> list.reverse,
        ", ",
      )
      <> "]"
  }

  // Generate param names for passing to _wc function (with labels)
  let param_names = case param_count {
    0 -> ""
    _ ->
      ", "
      <> string.join(
        int.range(from: 1, to: param_count + 1, with: [], run: fn(acc, n) {
          [
            case list.find(param_types, fn(pt) { pt.0 == n }) {
              Ok(#(_, col_name, _)) -> col_name <> ": " <> col_name
              Error(_) -> {
                let name = "p" <> int.to_string(n)
                name <> ": " <> name
              }
            },
            ..acc
          ]
        })
          |> list.reverse,
        ", ",
      )
  }

  // Strip comments and escape the SQL for Gleam string
  let escaped_sql = escape_string(strip_sql_comments(sql))
  let decoder_fn = case uses_model_decoder {
    True -> "row_decoder()"
    False -> snake_case(row_type_name) <> "_row_decoder()"
  }

  // Generate _wc (with connection) variant using db.query_with
  let wc_fn =
    generate_wc_query(
      fn_name,
      row_type_name,
      escaped_sql,
      param_values,
      decoder_fn,
      param_list,
      is_single_row,
    )

  // Generate main function that accepts Pool using get_connection
  let main_fn = case is_single_row {
    True ->
      "pub fn "
      <> fn_name
      <> "(pool pool: db.Pool"
      <> param_list
      <> ") -> Result("
      <> row_type_name
      <> ", db.DbError) {\n"
      <> "  use connection <- db.get_connection(pool)\n"
      <> "  "
      <> fn_name
      <> "_wc(connection: connection"
      <> param_names
      <> ")\n}"
    False ->
      "pub fn "
      <> fn_name
      <> "(pool pool: db.Pool"
      <> param_list
      <> ") -> Result(List("
      <> row_type_name
      <> "), db.DbError) {\n"
      <> "  use connection <- db.get_connection(pool)\n"
      <> "  "
      <> fn_name
      <> "_wc(connection: connection"
      <> param_names
      <> ")\n}"
  }

  string.join([main_fn, wc_fn], "\n\n")
}

/// The _wc function body differs between single-row and list
/// queries in an important way: single-row queries pattern
/// match on exactly [row] and return NotFound for empty
/// results, while list queries just pass all rows through. This
/// distinction means find_by_id returns a clear NotFound error
/// instead of an empty list that callers would have to check
/// manually.
///
fn generate_wc_query(
  fn_name: String,
  row_type_name: String,
  escaped_sql: String,
  param_values: String,
  decoder_fn: String,
  param_list: String,
  is_single_row: Bool,
) -> String {
  case is_single_row {
    True ->
      "pub fn "
      <> fn_name
      <> "_wc(connection connection: db.Connection"
      <> param_list
      <> ") -> Result("
      <> row_type_name
      <> ", db.DbError) {\n"
      <> "  case db.query_with(connection, \""
      <> escaped_sql
      <> "\", "
      <> param_values
      <> ", "
      <> decoder_fn
      <> ") {\n"
      <> "    Ok(db.QueryResult(_, [row])) -> Ok(row)\n"
      <> "    Ok(db.QueryResult(_, [])) -> Error(db.NotFound)\n"
      <> "    Ok(_) -> Error(db.QueryError(\"Expected single row\"))\n"
      <> "    Error(e) -> Error(e)\n"
      <> "  }\n}"
    False ->
      "pub fn "
      <> fn_name
      <> "_wc(connection connection: db.Connection"
      <> param_list
      <> ") -> Result(List("
      <> row_type_name
      <> "), db.DbError) {\n"
      <> "  case db.query_with(connection, \""
      <> escaped_sql
      <> "\", "
      <> param_values
      <> ", "
      <> decoder_fn
      <> ") {\n"
      <> "    Ok(db.QueryResult(_, rows)) -> Ok(rows)\n"
      <> "    Error(e) -> Error(e)\n"
      <> "  }\n}"
  }
}

/// INSERT, UPDATE, and DELETE don't return rows, so they use
/// db.exec_with instead of db.query_with — no decoder needed.
/// They still get the same Pool/_wc dual-function treatment so
/// you can use them inside transactions the same way you use
/// query functions.
///
fn generate_execute_function(
  fn_name: String,
  sql: String,
  params: List(Int),
  param_types: List(#(Int, String, ColumnType)),
) -> String {
  let param_count = list.length(params)

  // Generate parameter list with proper types and labeled names
  let param_list = case param_count {
    0 -> ""
    _ ->
      ", "
      <> string.join(
        int.range(from: 1, to: param_count + 1, with: [], run: fn(acc, n) {
          [
            case list.find(param_types, fn(pt) { pt.0 == n }) {
              Ok(#(_, col_name, col_type)) -> {
                let gleam_type = codegen.gleam_type(col_type)
                col_name <> " " <> col_name <> ": " <> gleam_type
              }
              Error(_) -> {
                let name = "p" <> int.to_string(n)
                name <> " " <> name <> ": String"
              }
            },
            ..acc
          ]
        })
          |> list.reverse,
        ", ",
      )
  }

  // Generate param names for passing to _wc function (with labels)
  let param_names = case param_count {
    0 -> ""
    _ ->
      ", "
      <> string.join(
        int.range(from: 1, to: param_count + 1, with: [], run: fn(acc, n) {
          [
            case list.find(param_types, fn(pt) { pt.0 == n }) {
              Ok(#(_, col_name, _)) -> col_name <> ": " <> col_name
              Error(_) -> {
                let name = "p" <> int.to_string(n)
                name <> ": " <> name
              }
            },
            ..acc
          ]
        })
          |> list.reverse,
        ", ",
      )
  }

  // Generate parameter values using db.int/db.string/etc.
  let param_values = case param_count {
    0 -> "[]"
    _ ->
      "["
      <> string.join(
        int.range(from: 1, to: param_count + 1, with: [], run: fn(acc, n) {
          [
            case list.find(param_types, fn(pt) { pt.0 == n }) {
              Ok(#(_, col_name, col_type)) -> {
                let wrapper = value_wrapper(col_type)
                wrapper <> "(" <> col_name <> ")"
              }
              Error(_) -> "db.string(p" <> int.to_string(n) <> ")"
            },
            ..acc
          ]
        })
          |> list.reverse,
        ", ",
      )
      <> "]"
  }

  // Strip comments and escape the SQL for Gleam string
  let escaped_sql = escape_string(strip_sql_comments(sql))

  // Generate _wc (with connection) variant using db.exec_with
  let wc_fn =
    generate_wc_execute(fn_name, escaped_sql, param_values, param_list)

  // Generate main function that accepts Pool using get_connection
  let main_fn =
    "pub fn "
    <> fn_name
    <> "(pool pool: db.Pool"
    <> param_list
    <> ") -> Result(Int, db.DbError) {\n"
    <> "  use connection <- db.get_connection(pool)\n"
    <> "  "
    <> fn_name
    <> "_wc(connection: connection"
    <> param_names
    <> ")\n}"

  string.join([main_fn, wc_fn], "\n\n")
}

/// The _wc execute variant returns the number of affected rows,
/// which is surprisingly useful — "0 rows updated" tells you
/// the WHERE clause matched nothing, saving you from doing a
/// separate SELECT to check if the row exists before deciding
/// to return a 404.
///
fn generate_wc_execute(
  fn_name: String,
  escaped_sql: String,
  param_values: String,
  param_list: String,
) -> String {
  "pub fn "
  <> fn_name
  <> "_wc(connection connection: db.Connection"
  <> param_list
  <> ") -> Result(Int, db.DbError) {\n"
  <> "  db.exec_with(connection, \""
  <> escaped_sql
  <> "\", "
  <> param_values
  <> ")\n}"
}

/// When the generated code passes parameters to the database,
/// it needs to wrap each value with the right db.int/db.string
/// call so the driver encodes it correctly. Getting this wrong
/// would mean silently sending "42" as a string to an integer
/// column, which might work on SQLite but would fail on
/// PostgreSQL.
///
fn value_wrapper(col_type: ColumnType) -> String {
  case col_type {
    schema_parser.Id -> "db.int"
    schema_parser.String -> "db.string"
    schema_parser.Text -> "db.string"
    schema_parser.Int -> "db.int"
    schema_parser.BigInt -> "db.int"
    schema_parser.Float -> "db.float"
    schema_parser.Boolean -> "db.bool"
    schema_parser.Timestamp -> "db.string"
    schema_parser.UnixTimestamp -> "db.int"
    schema_parser.Date -> "db.string"
    schema_parser.Json -> "db.string"
    schema_parser.Uuid -> "db.string"
    schema_parser.Foreign(_) -> "db.int"
  }
}

/// Table names come in as snake_case (like "user_profile") but
/// Gleam types must be PascalCase (like "UserProfile"). Without
/// this conversion the generated code wouldn't even compile.
///
fn pascal_case(s: String) -> String {
  s
  |> string.split("_")
  |> list.map(capitalize)
  |> string.join("")
}

/// Composite type names like "FindByNameUser" are PascalCase,
/// but their decoder functions need to be snake_case
/// ("find_by_name_user_row_decoder"). This does the reverse of
/// pascal_case so function names follow Gleam conventions.
///
fn snake_case(s: String) -> String {
  do_snake_case(string.to_graphemes(s), "", False)
}

/// Gleam doesn't have regex, so we walk the string character by
/// character and insert underscores at lowercase-to- uppercase
/// transitions. "FindByName" becomes "find_by_name" because the
/// underscore goes before each capital that follows a lowercase
/// letter.
///
fn do_snake_case(chars: List(String), acc: String, prev_lower: Bool) -> String {
  case chars {
    [] -> string.lowercase(acc)
    [c, ..rest] -> {
      let is_upper = c == string.uppercase(c) && c != string.lowercase(c)
      case is_upper && prev_lower {
        True -> do_snake_case(rest, acc <> "_" <> c, False)
        False -> do_snake_case(rest, acc <> c, !is_upper)
      }
    }
  }
}

/// Gleam's stdlib doesn't ship a capitalize function, so
/// pascal_case needs this to uppercase the first letter of each
/// word segment while leaving the rest alone — turning "user"
/// into "User" and "profile" into "Profile".
///
fn capitalize(s: String) -> String {
  case string.pop_grapheme(s) {
    Ok(#(first, rest)) -> string.uppercase(first) <> rest
    Error(_) -> s
  }
}

/// SQL often contains quotes (for string comparisons) and
/// sometimes backslashes. If we embedded those raw into the
/// generated Gleam string literal, the generated file wouldn't
/// compile. Newlines also need to become spaces since the SQL
/// goes into a single-line string.
///
fn escape_string(s: String) -> String {
  s
  |> string.replace("\\", "\\\\")
  |> string.replace("\"", "\\\"")
  |> string.replace("\n", " ")
}

/// Developers write comments in their .sql query files to
/// explain the logic, but those comments shouldn't end up in
/// the generated Gleam code — they'd clutter the output and
/// make the embedded SQL strings unnecessarily long. This
/// strips both block and line comments before embedding.
///
fn strip_sql_comments(sql: String) -> String {
  sql
  |> strip_block_comments()
  |> strip_line_comments()
  |> collapse_whitespace()
}

/// Block comments (/* ... */) can span multiple lines and even
/// nest in edge cases. Recursing after each removal handles the
/// case where stripping one comment reveals another that was
/// partially inside it — unlikely in practice but easy to
/// handle correctly.
///
fn strip_block_comments(sql: String) -> String {
  case string.split_once(sql, "/*") {
    Error(_) -> sql
    Ok(#(before, after)) -> {
      case string.split_once(after, "*/") {
        Error(_) -> before
        Ok(#(_, rest)) -> strip_block_comments(before <> " " <> rest)
      }
    }
  }
}

/// Line comments (-- ...) run to end of line, so splitting on
/// newlines first and then truncating at "--" cleanly removes
/// them. This is simpler than trying to handle them with the
/// block comment logic since they have completely different
/// termination rules.
///
fn strip_line_comments(sql: String) -> String {
  sql
  |> string.split("\n")
  |> list.map(fn(line) {
    case string.split_once(line, "--") {
      Error(_) -> line
      Ok(#(before, _)) -> before
    }
  })
  |> string.join(" ")
}

/// After stripping comments and joining lines, you can end up
/// with runs of multiple spaces where comments used to be.
/// Recursively collapsing double-spaces to single-spaces
/// produces clean, compact SQL that's easier to read if you
/// ever inspect the generated file.
///
fn collapse_whitespace(sql: String) -> String {
  let collapsed = string.replace(sql, "  ", " ")
  case collapsed == sql {
    True -> string.trim(sql)
    False -> collapse_whitespace(collapsed)
  }
}
